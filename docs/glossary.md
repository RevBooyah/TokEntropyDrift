# Glossary of Terms: TokEntropyDrift

This document defines key terms used throughout the TokEntropyDrift project, especially those related to tokenization, entropy analysis, and compression metrics.

---

### üî§ Tokenization Terms

* **Token**: A subword, character, or word-like unit generated by a tokenizer.
* **Tokenizer**: An algorithm or model that segments text into tokens, e.g., BPE, WordPiece, SentencePiece.
* **Subword**: A partial word often used in tokenization to reduce vocabulary size and handle out-of-vocab words.
* **Byte-Pair Encoding (BPE)**: A compression-inspired tokenization technique that iteratively merges the most frequent pairs of symbols.
* **SentencePiece**: A language-agnostic tokenizer based on Unigram or BPE models, often used in multilingual models.
* **WordPiece**: A tokenization algorithm that builds subword units based on maximum likelihood training.
* **Token Boundary**: The start and end position of a token within the original input text.
* **UNK Token**: A special token used to indicate an out-of-vocabulary (unknown) token.
* **Token Drift**: The difference in token boundaries or counts when comparing multiple tokenizers on the same input.
* **Tokenizer Alignment**: A mapping or overlay that shows how two or more tokenizers segment the same input.

---

### üìè Metrics & Analysis Terms

* **Token Count**: The total number of tokens output for a given text input.
* **Token Reuse Rate**: The ratio of unique tokens to total tokens. Lower reuse implies more repetition.
* **Average Token Length**: The average number of characters per token in a given sample.
* **Compression Ratio**: The ratio of input text length (in bytes or characters) to the number of output tokens.
* **Token Density**: Number of tokens per character or byte.
* **Entropy**: A measure of unpredictability or information content based on token frequency distribution.
* **Shannon Entropy**: The sum over all token probabilities of `-p * log2(p)`, indicating average information per token.
* **Rolling Entropy**: Entropy calculated over a sliding window of tokens, revealing local complexity.
* **Conditional Entropy**: Entropy of a token given the previous one (e.g., bigram-based entropy).
* **Entropy Drift**: The difference in entropy between tokenizers on the same input.

---

### üß™ Visualization & Output Terms

* **Heatmap**: A graphical representation where color intensity indicates metric magnitude (e.g., token count or entropy).
* **Boundary Overlay**: A layered visualization showing how multiple tokenizers split the same input.
* **Token Stream Plot**: A timeline view of token progression across input, highlighting length, position, or frequency.
* **Sankey Diagram**: A potential future visualization showing flow or mapping between token sequences.
* **Export Format**: The output structure used for saving results (e.g., CSV, JSON, Markdown, LaTeX).

---

### ‚öôÔ∏è Platform & CLI Terms

* **ted**: The command-line tool that runs analysis and visualization routines.
* **Config File**: A YAML or TOML file specifying default inputs, tokenizers, and analysis parameters.
* **Module**: A pluggable component responsible for a specific analysis or transformation.
* **Plugin**: A user-registered external script or adapter for custom tokenizers or metrics.
* **Batch Mode**: Processing multiple inputs at once, typically from a file or corpus.
* **Headless Mode**: Running analysis or generating visualizations without an interactive UI (e.g., for CI pipelines).

---

This glossary will expand as the system grows and incorporates more tokenizer behaviors, model correlations, and visualization types.
